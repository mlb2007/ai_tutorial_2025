{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212b5cc2",
   "metadata": {},
   "source": [
    "# YouTube Transcript Agent with LangChain & OpenAI as LLM\n",
    "\n",
    "This notebook demonstrates how to use a LangChain agent with OpenAI as the LLM and a custom YouTube transcript tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff0274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies (uncomment and run if needed)\n",
    "# !pip install langchain langchain_community youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91607063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import re\n",
    "import os\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "#Set up the Ollama LLM (using updated langchain_ollama package)\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langchain_core.tools import tool as langchain_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9575b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../.env\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY_PERSONAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a185321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm(model_name: str = \"llama3.2:latest\") -> OllamaLLM:\n",
    "    \"\"\"\n",
    "    Pure function to create an OllamaLLM instance.\n",
    "    Args:\n",
    "        model_name (str): The name of the Ollama model to use.\n",
    "    Returns:\n",
    "        OllamaLLM: An instance of the OllamaLLM.\n",
    "    \"\"\"\n",
    "    return OllamaLLM(model=model_name)\n",
    "#model_ollama = create_llm()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7755f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, AuthenticationError, APIConnectionError, APIError\n",
    "\n",
    "def diagnose_openai_connection(model):\n",
    "    \"\"\"\n",
    "    Diagnoses possible causes for OpenAI API 429 errors, including quota, IP block, and network issues.\n",
    "    Returns a dict with diagnostic results.\n",
    "    \"\"\"\n",
    "    diagnostics = {}\n",
    "\n",
    "    # 1. Check if API key is valid\n",
    "    try:\n",
    "        # This will fail fast if the key is invalid\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        client.models.list()\n",
    "        diagnostics['api_key_valid'] = True\n",
    "    except AuthenticationError as e:\n",
    "        diagnostics['api_key_valid'] = False\n",
    "        diagnostics['error'] = f\"AuthenticationError: {e}\"\n",
    "        return diagnostics\n",
    "    except Exception as e:\n",
    "        diagnostics['api_key_valid'] = False\n",
    "        diagnostics['error'] = f\"Unexpected error during API key check: {e}\"\n",
    "        return diagnostics\n",
    "\n",
    "    # 2. Check if IP is blocked (simulate by calling a simple endpoint)\n",
    "    try:\n",
    "        with httpx.Client(timeout=10.0) as client:\n",
    "            response = client.get(\n",
    "                \"https://api.openai.com/v1/models\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"\n",
    "                }\n",
    "            )\n",
    "        diagnostics['http_status'] = response.status_code\n",
    "        if response.status_code == 429:\n",
    "            diagnostics['ip_blocked_or_rate_limited'] = True\n",
    "        else:\n",
    "            diagnostics['ip_blocked_or_rate_limited'] = False\n",
    "    except Exception as e:\n",
    "        diagnostics['ip_blocked_or_rate_limited'] = None\n",
    "        diagnostics['error'] = f\"Network error: {e}\"\n",
    "        return diagnostics\n",
    "\n",
    "    # 3. Try a simple model invocation and catch errors\n",
    "    try:\n",
    "        result = model.invoke(\"Hello, how are you?\")\n",
    "        diagnostics['model_invoke_success'] = True\n",
    "        diagnostics['result'] = str(result)\n",
    "    except RateLimitError as e:\n",
    "        diagnostics['model_invoke_success'] = False\n",
    "        diagnostics['error'] = f\"RateLimitError: {e}\"\n",
    "    except APIConnectionError as e:\n",
    "        diagnostics['model_invoke_success'] = False\n",
    "        diagnostics['error'] = f\"APIConnectionError: {e}\"\n",
    "    except APIError as e:\n",
    "        diagnostics['model_invoke_success'] = False\n",
    "        diagnostics['error'] = f\"APIError: {e}\"\n",
    "    except Exception as e:\n",
    "        diagnostics['model_invoke_success'] = False\n",
    "        diagnostics['error'] = f\"Other error: {e}\"\n",
    "\n",
    "    return diagnostics\n",
    "\n",
    "def print_diagnostics(diagnostics):\n",
    "    print(\"OpenAI API Diagnostics:\")\n",
    "    for k, v in diagnostics.items():\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0943997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Diagnose OpenAI connection\n",
    "#diagnostics = diagnose_openai_connection(model)\n",
    "#print_diagnostics(diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83500f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pure functional YouTube transcript tool\n",
    "@langchain_tool\n",
    "def fetch_youtube_transcript(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract transcript with timestamps from a YouTube video URL and format it for LLM consumption.\n",
    "    Args:\n",
    "        url (str): YouTube video URL\n",
    "    Returns:\n",
    "        str: Formatted transcript with timestamps, each entry on a new line as \"[MM:SS] Text\"\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from typing import Any\n",
    "\n",
    "    def extract_video_id(url: str) -> str:\n",
    "        video_id_pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\n",
    "        video_id_match = re.search(video_id_pattern, url)\n",
    "        if not video_id_match:\n",
    "            raise ValueError(\"Invalid YouTube URL\")\n",
    "        return video_id_match.group(1)\n",
    "\n",
    "    def format_entry(entry: Any) -> str:\n",
    "        # Try attribute access, fallback to dict-style if needed\n",
    "        start = getattr(entry, 'start', None)\n",
    "        text = getattr(entry, 'text', None)\n",
    "        if start is None or text is None:\n",
    "            # fallback to dict-style (for older API versions)\n",
    "            start = entry['start']\n",
    "            text = entry['text']\n",
    "        return f\"[{int(start//60):02d}:{int(start%60):02d}] {text}\"\n",
    "\n",
    "    video_id = extract_video_id(url)\n",
    "    transcript = YouTubeTranscriptApi().fetch(video_id=video_id, languages=['en', 'ta'])\n",
    "    formatted_entries = list(map(format_entry, transcript))\n",
    "    return \"\\n\".join(formatted_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145993bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(fetch_youtube_transcript))\n",
    "print(hasattr(fetch_youtube_transcript, \"invoke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac1710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [fetch_youtube_transcript]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525c7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript test passed.\n"
     ]
    }
   ],
   "source": [
    "# 8. (Optional) Test for the tool\n",
    "def test_fetch_youtube_transcript():\n",
    "    url = \"https://www.youtube.com/watch?v=ZaY5_ScmiFE\"\n",
    "    transcript = fetch_youtube_transcript.invoke(url)\n",
    "    assert isinstance(transcript, str)\n",
    "    assert \"[00:\" in transcript\n",
    "    print(\"Transcript test passed.\")\n",
    "\n",
    "# Uncomment to run the test\n",
    "test_fetch_youtube_transcript()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d5fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(state: MessagesState):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that can run python code.\"),\n",
    "    ] + state[\"messages\"]\n",
    "    return {\"messages\": [model_with_tools.invoke(messages)]}\n",
    "\n",
    "\n",
    "def call_tool(state: MessagesState):\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"environment\", \"END\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    return \"END\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ba2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "agent_builder.add_node(\"llm\", call_llm)\n",
    "agent_builder.add_node(\"environment\", call_tool)\n",
    "\n",
    "agent_builder.add_edge(START, \"llm\")\n",
    "# the node to be called next is determined by the function should_continue()\n",
    "# whose return statement is either \"Action\" or END and this is used to determine the next node to be called\n",
    "# if the return statement is \"Action\", then the node \"environment\" is called\n",
    "# if the return statement is END, then the agent is terminated\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"Action\": \"environment\",\n",
    "        \"END\": END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment\", \"llm\")\n",
    "\n",
    "agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20f71fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aee9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "def build_transcript_messages(\n",
    "    url: str, \n",
    "    start_time: str = None, \n",
    "    end_time: str = None, \n",
    "    translation: bool = False\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Builds the messages required for the agent to generate a transcript for a given YouTube URL.\n",
    "    If translation is True, requests translation into English.\n",
    "    Args:\n",
    "        url (str): The YouTube video URL.\n",
    "        start_time (str, optional): Start time for transcript.\n",
    "        end_time (str, optional): End time for transcript.\n",
    "        translation (bool, optional): Whether to translate transcript into English.\n",
    "    Returns:\n",
    "        List: A list of SystemMessage and HumanMessage objects.\n",
    "    \"\"\"\n",
    "    base_instruction = (\n",
    "        f\"Generate a transcript for the video given by the user at url:{url}\"\n",
    "        + (\n",
    "            f\" from {start_time} to {end_time}.\"\n",
    "            if start_time is not None and end_time is not None\n",
    "            else \".\"\n",
    "        )\n",
    "    )\n",
    "    if translation:\n",
    "        base_instruction += \" Translate the transcript into English if it is not already in English.\"\n",
    "    return [\n",
    "        SystemMessage(content=\"You are a helpful assistant that can run transcribe youtube URL videos.\"),\n",
    "        HumanMessage(content=base_instruction),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec390da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Make sure the URL vide has subtitles turned on ... else this will not be able to extract data!\n",
    "url = \"https://www.youtube.com/watch?v=ZaY5_ScmiFE\"\n",
    "#url = 'https://www.youtube.com/watch?v=6zDU9JlJp_0'\n",
    "#url = 'https://www.youtube.com/watch?v=mgC8jO7aAMM&t=1160s'\n",
    "messages = build_transcript_messages(url, start_time=\"00:00\", end_time=\"00:15\", translation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed976453",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdc0c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant that can run transcribe youtube URL videos.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Generate a transcript for the video given by the user at url:https://www.youtube.com/watch?v=ZaY5_ScmiFE from 00:00 to 00:15. Translate the transcript into English if it is not already in English.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  fetch_youtube_transcript (call_KjsxSE0r6BZaCZNKNDNtDpu0)\n",
      " Call ID: call_KjsxSE0r6BZaCZNKNDNtDpu0\n",
      "  Args:\n",
      "    url: https://www.youtube.com/watch?v=ZaY5_ScmiFE\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "[00:00] Hey everyone, I'm Shaw. This is the\n",
      "[00:02] first video in a larger series on AI\n",
      "[00:05] agents. Here I'll start by discussing\n",
      "[00:07] what an AI agent actually is and why we\n",
      "[00:10] should care about them. Then I'll review\n",
      "[00:12] three common types of agents at various\n",
      "[00:15] levels of agency. 2025 is said to be the\n",
      "[00:19] year of AI agents. Yet for many people\n",
      "[00:22] that I've talked to, it's not entirely\n",
      "[00:25] clear what this actually means. Part of\n",
      "[00:28] the problem is that no one can seem to\n",
      "[00:31] agree on a single definition for AI\n",
      "[00:35] agents. To demonstrate this, here are\n",
      "[00:36] some definitions from a few leading\n",
      "[00:39] organizations. Open AAI in their agents\n",
      "[00:41] SDK documentation defines an agent as a\n",
      "[00:45] large language model configured with\n",
      "[00:47] instructions and tools. This is\n",
      "[00:49] different from say hugging faces\n",
      "[00:51] definition from their transformers\n",
      "[00:54] documentation which describes it as a\n",
      "[00:56] system where a large language model can\n",
      "[00:58] execute more complex tasks through\n",
      "[01:01] planning and using tools. So here\n",
      "[01:04] there's a subtle difference. Open AI's\n",
      "[01:06] definition is focused on tools. That's\n",
      "[01:08] like the main thing. While hugging face\n",
      "[01:10] they talk about tools but they also\n",
      "[01:12] mention planning which is basically\n",
      "[01:14] taking a user's query or an input\n",
      "[01:17] breaking the task down or the problem\n",
      "[01:20] down into specific steps and then\n",
      "[01:23] executing it. And then if we look at\n",
      "[01:24] anthropics definition and so this is\n",
      "[01:26] from a blog post called building\n",
      "[01:28] effective agents where they define it as\n",
      "[01:31] systems where LLMs dynamically direct\n",
      "[01:34] their own processes and tool usage\n",
      "[01:36] maintaining control over how they\n",
      "[01:38] accomplish tasks. So again we see this\n",
      "[01:40] mention of tool usage. However,\n",
      "[01:42] anthropics definition seems to be\n",
      "[01:44] primarily focused on autonomy. Basically\n",
      "[01:48] LLM systems that have complete control\n",
      "[01:51] over how they do things. So, I won't try\n",
      "[01:54] to make things even worse by proposing\n",
      "[01:56] yet another definition, but instead I\n",
      "[01:58] want to talk about three key features\n",
      "[02:00] that come up regardless of the specific\n",
      "[02:03] definition that people might give to\n",
      "[02:06] agents. So, first and foremost, there's\n",
      "[02:08] always an LLM involved. So, in those\n",
      "[02:10] three definitions we saw in the previous\n",
      "[02:12] slide and all the other definitions I've\n",
      "[02:14] seen, LLMs play a central role in what\n",
      "[02:18] people are calling AI agents today. So,\n",
      "[02:20] it's going to be an LLM represented by\n",
      "[02:22] this robot emoji and then some other\n",
      "[02:24] stuff, some software you write around\n",
      "[02:27] it. Maybe you use a framework or you\n",
      "[02:28] code it from scratch. But there's some\n",
      "[02:30] other stuff that kind of augments the\n",
      "[02:32] LLM. The next thing is tool use. So,\n",
      "[02:35] this can be something like a web search,\n",
      "[02:36] a Python interpreter, memory, sending an\n",
      "[02:40] email, or even access to a different\n",
      "[02:42] large language model that is specialized\n",
      "[02:44] for a particular task. And I'll talk\n",
      "[02:46] about why tool usage is so important for\n",
      "[02:49] agents on the next slide. And then\n",
      "[02:51] finally there is usually some mention of\n",
      "[02:55] autonomy. So we saw in hugging faces\n",
      "[02:58] definition there was this notion of\n",
      "[03:00] planning and anthropics definition which\n",
      "[03:02] we'll talk more about. They really\n",
      "[03:04] stress the importance of these like open\n",
      "[03:07] loops. This ability for the model to\n",
      "[03:11] reflect on its outputs and get real\n",
      "[03:14] world feedback on them. Another thing\n",
      "[03:16] this might include is the so-called\n",
      "[03:18] reasoning capabilities of LLMs like\n",
      "[03:21] DeepSeek R1 or OpenAI's 01 or 03 models.\n",
      "[03:26] This ability for LLMs to quote unquote\n",
      "[03:28] think before providing an answer allows\n",
      "[03:31] it to accomplish more and more complex\n",
      "[03:33] tasks without explicit instructions. And\n",
      "[03:36] so everyone is very excited about agents\n",
      "[03:39] these days. People are saying 2025 is\n",
      "[03:41] the year of AI agents. YC is saying that\n",
      "[03:43] vertical AI agents could be 10 times\n",
      "[03:46] bigger than SAS. And this might be very\n",
      "[03:48] reminiscent to a couple years ago when\n",
      "[03:50] chatbt came out and there was a lot of\n",
      "[03:53] excitement around LLMs and specifically\n",
      "[03:55] building chatbots. Back then I had\n",
      "[03:57] people reaching out to me all the time\n",
      "[03:59] saying how they wanted to build some\n",
      "[04:00] kind of chatbot for their company for\n",
      "[04:02] some specific use case. And then fast\n",
      "[04:05] forward one year later, a sentiment I\n",
      "[04:06] heard from a lot of business leaders or\n",
      "[04:09] business operators is that they weren't\n",
      "[04:11] able to get ROI from these chatbots. And\n",
      "[04:14] so they might be thinking it's just a\n",
      "[04:16] repeat of this is just like another hype\n",
      "[04:18] thing that isn't going to generate\n",
      "[04:20] return on investment. While there's a\n",
      "[04:22] lot to unpack there, the key difference\n",
      "[04:24] between agents and chat bots is that\n",
      "[04:26] LLMs alone aren't enough to solve most\n",
      "[04:30] real world problems. And so there are\n",
      "[04:32] two key things that I would say make\n",
      "[04:33] agents much more valuable than just\n",
      "[04:36] vanilla chat bots like the original\n",
      "[04:38] version of chat GBT. So the first is\n",
      "[04:40] that agents can interact with reality\n",
      "[04:43] through tool use. LLMs just out of the\n",
      "[04:46] box they don't live in reality. They\n",
      "[04:48] live in an imagined reality. They live\n",
      "[04:51] in some kind of imagination land. This\n",
      "[04:53] is why hallucinations are a big problem\n",
      "[04:55] with LLMs. And then if you asked one of\n",
      "[04:57] these LLMs to write you some Python code\n",
      "[04:59] and execute it, it would just make up\n",
      "[05:02] the outputs. It wouldn't actually run\n",
      "[05:04] the code because LLMs can only generate\n",
      "[05:06] text. They can't run code. They can't do\n",
      "[05:09] web search. They can't do a lot of\n",
      "[05:10] useful things to get feedback from the\n",
      "[05:13] real world to make them more useful. So\n",
      "[05:16] tools take LLMs from their imagination\n",
      "[05:20] land based on their pre-training data\n",
      "[05:22] and put them in the real world. kind of\n",
      "[05:25] give them real world feedback, real\n",
      "[05:27] world context. Another reason why agents\n",
      "[05:29] are inherently more valuable than the\n",
      "[05:32] vanilla chat bots we've seen in the past\n",
      "[05:34] is this discovery of so-called testime\n",
      "[05:37] compute scaling. And this is just a\n",
      "[05:39] fancy way of saying that the more tokens\n",
      "[05:41] an LLM generates, the better its\n",
      "[05:45] responses. So this is the whole idea of\n",
      "[05:47] giving models time to think about\n",
      "[05:50] problems, allowing them to kind of\n",
      "[05:52] reason about problems, come up with an\n",
      "[05:54] action plan before giving their final\n",
      "[05:57] answer. And so today with models like 01\n",
      "[05:59] and Deepseek R1 which are trained using\n",
      "[06:01] reinforcement learning, now we have\n",
      "[06:03] models which are much more capable of\n",
      "[06:05] solving arbitrary tasks without explicit\n",
      "[06:08] instructions than before. And this\n",
      "[06:11] ability to just oneshot novel problems\n",
      "[06:14] is actually very valuable for agents\n",
      "[06:17] because now you don't have to write very\n",
      "[06:19] sophisticated and detailed prompts for\n",
      "[06:22] specific problems. These models can\n",
      "[06:24] solve a wider range of tasks. Well, and\n",
      "[06:27] if you're not familiar with this test\n",
      "[06:28] time compute and you know reasoning\n",
      "[06:30] models and all that, I talked all about\n",
      "[06:32] that in a previous video which I'll link\n",
      "[06:33] here in case you are interested. Since\n",
      "[06:36] no one can seem to agree on a single\n",
      "[06:38] definition for AI agents, most\n",
      "[06:40] practitioners I talk to don't really\n",
      "[06:42] worry so much if a system is an AI agent\n",
      "[06:45] or not, but rather they just talk about\n",
      "[06:47] agentic systems. Basically conveying\n",
      "[06:50] this idea that agency is on a spectrum.\n",
      "[06:53] In other words, you can have systems\n",
      "[06:54] with no agency. So basically like a\n",
      "[06:57] rule-based system where the logic is\n",
      "[06:59] hard-coded all the way to a system that\n",
      "[07:02] has human level agency. And so here I'm\n",
      "[07:05] going to convey this idea by talking\n",
      "[07:07] about three systems at increasing levels\n",
      "[07:10] of agency. So level one is an LLM plus\n",
      "[07:14] tools. Level two are LLM workflows and\n",
      "[07:17] level three is an LLM in a loop. So tool\n",
      "[07:20] use is a central feature of any agentic\n",
      "[07:23] system. And the main thing here is that\n",
      "[07:25] tools expand the capabilities of LLMs\n",
      "[07:28] beyond text completion. Two common tools\n",
      "[07:31] that you might give an LLM are web\n",
      "[07:33] search and a code interpreter. And so\n",
      "[07:36] these two tools kind of fill in two\n",
      "[07:39] major blind spots of an LLM. For LLMs\n",
      "[07:42] out of the box, their understanding of\n",
      "[07:44] the world is based on a snapshot from\n",
      "[07:47] like 12 months ago, which was captured\n",
      "[07:49] by their pre-training data. And of\n",
      "[07:52] course, the world has changed a lot over\n",
      "[07:53] the past 12 months or so. So if you were\n",
      "[07:56] to ask an LLM about anything recent, it\n",
      "[07:58] would basically just make something up.\n",
      "[08:00] This is where web search can help. You\n",
      "[08:02] can give the model access to a tool like\n",
      "[08:06] web search and then you can train it to\n",
      "[08:08] identify when it receives requests where\n",
      "[08:12] web search is going to be a helpful\n",
      "[08:15] thing to do. Another really handy tool\n",
      "[08:17] is a code interpreter. This allows LLM\n",
      "[08:20] to do a lot of different tasks. for\n",
      "[08:23] example, doing math, which is something\n",
      "[08:25] that LLMs really struggle with. Instead\n",
      "[08:27] of basically the LLM trying to do the\n",
      "[08:30] math in its head, it can write some\n",
      "[08:31] Python code to do some complicated\n",
      "[08:33] arithmetic and it can send that code to\n",
      "[08:36] its Python interpreter. The code can\n",
      "[08:38] actually be run and the results can be\n",
      "[08:40] sent back to the LLM. And no, without\n",
      "[08:42] the code interpreter, the LLM would\n",
      "[08:44] still be able to write code. It just\n",
      "[08:46] wouldn't be able to run it. So, it would\n",
      "[08:48] basically make up the outputs. And so\n",
      "[08:50] these are two very fundamental tools,\n",
      "[08:52] but there are many others that you could\n",
      "[08:54] use. So another big one are API calls.\n",
      "[08:57] So maybe you want to fetch information\n",
      "[08:59] from YouTube or you might want to\n",
      "[09:01] connect to Gmail. So maybe you want the\n",
      "[09:03] LLM to be able to read your emails or to\n",
      "[09:05] write emails on your behalf. Or even you\n",
      "[09:08] might have like a notion page where you\n",
      "[09:11] have a lot of notes and you want the LLM\n",
      "[09:13] to be able to access those notes and do\n",
      "[09:15] some kind of task that is helpful to\n",
      "[09:17] you. And so notice the API calls go both\n",
      "[09:19] ways. They can both be used to give the\n",
      "[09:22] LLM real world context, real world\n",
      "[09:25] information, and they can also be used\n",
      "[09:28] to allow the model to take actions in\n",
      "[09:31] the real world, which are two essential\n",
      "[09:33] components of agency. Another popular\n",
      "[09:35] one is computer use. Not every program\n",
      "[09:38] you care about has an API, basically has\n",
      "[09:41] a programmatic way of interacting with\n",
      "[09:43] it, but it does have a graphical user\n",
      "[09:45] interface. So computer use enables LLMs\n",
      "[09:49] to interact with these graphical\n",
      "[09:51] interfaces using mouse clicks and\n",
      "[09:53] keyboard strokes. Another tool could be\n",
      "[09:55] another model. So for example, ChadByt\n",
      "[09:58] has access to a text to image model. So\n",
      "[10:01] if you ask it to create an image for\n",
      "[10:03] you, it can take that request, pass it\n",
      "[10:06] to this external model, and then take\n",
      "[10:08] that resulting image and show it to you.\n",
      "[10:10] Another tool might be rag. So instead of\n",
      "[10:13] searching the internet, maybe you just\n",
      "[10:15] want to search an internal knowledge\n",
      "[10:17] base or over a set of documents or\n",
      "[10:20] something like that. So that's where RAD\n",
      "[10:21] can be helpful. And then another helpful\n",
      "[10:23] tool is memory, which allows the system\n",
      "[10:26] to retain important information over a\n",
      "[10:29] long time period. So this could be over\n",
      "[10:32] multiple LLM calls or even over multiple\n",
      "[10:35] conversations. So, while this level one\n",
      "[10:38] agent of LLM plus tools matches OpenAI's\n",
      "[10:41] definition, many people I've talked to\n",
      "[10:43] wouldn't consider this a quote unquote\n",
      "[10:46] AI agent, the core limitation of systems\n",
      "[10:49] like this is that you're basically\n",
      "[10:51] relying on a single model and a single\n",
      "[10:54] thread of information to solve whatever\n",
      "[10:57] task you're trying to do. And so, while\n",
      "[10:59] this works fine for one-off tasks or\n",
      "[11:01] small tasks, as the requests get more\n",
      "[11:04] complicated, this can start to break\n",
      "[11:05] down. Like for example, if I wanted a\n",
      "[11:08] system to like end to end write blogs\n",
      "[11:10] for me, so basically come up with blog\n",
      "[11:12] ideas, pick out the best one, write a\n",
      "[11:15] first draft or something like that. This\n",
      "[11:17] might be hard for a single model to do\n",
      "[11:20] well, even if it does have access to\n",
      "[11:22] like web search to do research and a\n",
      "[11:24] library of like past blogs and past\n",
      "[11:27] topics or something like that. This\n",
      "[11:28] brings us to level two, which are LLM\n",
      "[11:31] workflows. And so most of the agentic\n",
      "[11:34] systems that you'll see these days are\n",
      "[11:37] this kind of system. So frameworks like\n",
      "[11:40] langraph and llama index are really\n",
      "[11:42] optimized for building these kinds of\n",
      "[11:44] agentic systems. And so all a LLM\n",
      "[11:47] workflow is is a predefined sequence of\n",
      "[11:49] steps with at least one LLM involved. To\n",
      "[11:52] give a concrete example, let's say I\n",
      "[11:54] wanted an agentic system, an AI agent to\n",
      "[11:57] help me respond to emails. Instead of\n",
      "[12:00] just taking an LLM, giving it very\n",
      "[12:02] detailed instructions, and giving it\n",
      "[12:04] access to my Gmail, like the level one\n",
      "[12:06] system, I might define a workflow like\n",
      "[12:08] this to kind of make the system a bit\n",
      "[12:11] more reliable. So, the workflow would\n",
      "[12:13] look something like this. An email comes\n",
      "[12:15] into my inbox. Then, this LLM would spit\n",
      "[12:17] up and it would categorize the email.\n",
      "[12:20] Basically, it would see if it's junk or\n",
      "[12:22] not junk. If it's junk, the system would\n",
      "[12:24] move it to the trash. If it's not junk,\n",
      "[12:26] I could pass it to another LLM to decide\n",
      "[12:29] if it's a hard email to respond to or\n",
      "[12:32] just like a easy email to respond to.\n",
      "[12:35] So, if it's just like a sounds great,\n",
      "[12:36] thanks, or an email that I've written\n",
      "[12:39] many times before, and there's like a\n",
      "[12:41] template for it or something like that,\n",
      "[12:42] it can make that decision. Notice we\n",
      "[12:44] could have just had one LLM do this\n",
      "[12:46] categorization where we have one LLM\n",
      "[12:48] that decides whether the email is junk,\n",
      "[12:51] an easy response, or hard response. But\n",
      "[12:54] this is a more complicated task. So it's\n",
      "[12:56] going to be harder for a LLM to do in\n",
      "[12:59] just one shot. So by splitting this\n",
      "[13:01] complex task into two simpler tasks, we\n",
      "[13:04] actually make it easier for each of\n",
      "[13:06] these LLMs. And maybe you don't even\n",
      "[13:08] need to use an LLM here. Maybe you can\n",
      "[13:10] just use a fine-tuned text classifier to\n",
      "[13:12] do the junk not junk. But then maybe\n",
      "[13:14] this categorization is a bit more\n",
      "[13:16] difficult and harder to gather training\n",
      "[13:18] data for. So an LLM is better suited\n",
      "[13:21] here. Okay. But then going on with the\n",
      "[13:22] workflow, if it's a hard response, the\n",
      "[13:24] system can just leave it for me to\n",
      "[13:26] respond to. If it's an easy response, it\n",
      "[13:28] can get passed to a LLM email writer\n",
      "[13:31] that specializes in this, and then that\n",
      "[13:33] LLM can send an email. And so the key\n",
      "[13:36] difference between level one and level\n",
      "[13:38] two is that we're no longer limited to\n",
      "[13:39] just a single LLM with access to some\n",
      "[13:42] tools, but rather we can have multiple\n",
      "[13:45] of these LLM plus tool systems working\n",
      "[13:48] together to solve more and more complex\n",
      "[13:50] tasks with better performance. And so\n",
      "[13:53] there are really countless ways you can\n",
      "[13:55] design LLM workflows. Some are going to\n",
      "[13:57] have more agency than others, but\n",
      "[13:59] there's this really nice blog post by\n",
      "[14:01] Anthropic. It's reference number three.\n",
      "[14:03] It's called Building Effective Agents.\n",
      "[14:05] And there they break down various common\n",
      "[14:08] design patterns for these workflows. So\n",
      "[14:10] I just wanted to mention those here. So\n",
      "[14:12] the first one is chaining. So this is\n",
      "[14:14] like a very simple design. You have\n",
      "[14:16] module A passes output to module B which\n",
      "[14:18] passes output to module C. So maybe you\n",
      "[14:21] have a system that is going to write\n",
      "[14:22] blog posts for you. So you have one\n",
      "[14:25] module just coming up with ideas.\n",
      "[14:27] Another module deciding what ideas are\n",
      "[14:29] good and what ideas are bad. And then\n",
      "[14:31] you have a third module that writes the\n",
      "[14:33] first draft of that blog post. And\n",
      "[14:35] again, you could probably have a single\n",
      "[14:37] model do this, but it probably wouldn't\n",
      "[14:39] perform as well as a system that breaks\n",
      "[14:41] it into more specialized roles. Another\n",
      "[14:44] common design is routing. So this is\n",
      "[14:47] something we saw in the example I shared\n",
      "[14:49] on the previous slide where you have\n",
      "[14:50] like this classification step and a fork\n",
      "[14:53] in the workflow. So let's say you have\n",
      "[14:55] an email come in and you want to\n",
      "[14:57] classify it as junk and not junk. And\n",
      "[15:00] that's because you want to treat these\n",
      "[15:01] two different types of inputs\n",
      "[15:03] differently. Another common design is\n",
      "[15:05] what Anthropic called parallelization.\n",
      "[15:08] So there are two kinds of\n",
      "[15:09] parallelization. The first one is\n",
      "[15:10] sectioning. Sectioning is when you take\n",
      "[15:12] a task and you split it into various\n",
      "[15:16] subtasks that can actually be run at the\n",
      "[15:18] same time. A good example of this is say\n",
      "[15:20] a user types in a request into a chat\n",
      "[15:23] interface and let's say to process this\n",
      "[15:25] request you need two things to happen.\n",
      "[15:28] one, you need the LLM to generate a\n",
      "[15:30] response, and two, you need to make sure\n",
      "[15:32] the request is appropriate and abides by\n",
      "[15:35] your terms of use or some set of rules\n",
      "[15:38] that you have for your application. So,\n",
      "[15:41] you can actually run these tasks at the\n",
      "[15:43] same time because they're not dependent\n",
      "[15:45] on each other. And this will result in\n",
      "[15:47] lower latency because you don't have to\n",
      "[15:49] check if the query abides by the rules\n",
      "[15:51] and then generate a response to it. You\n",
      "[15:53] can just do these at the same time and\n",
      "[15:55] then generate the response. Another\n",
      "[15:57] version of parallelization is voting. So\n",
      "[16:00] let's say you have some kind of lead\n",
      "[16:02] scoring system. You want to grade leads\n",
      "[16:04] as like lowquality, medium quality, high\n",
      "[16:07] quality or something like that. But this\n",
      "[16:09] is a pretty hard task where a single LLM\n",
      "[16:12] might get it wrong like 60 or 70% of the\n",
      "[16:15] time. But one way you can kind of\n",
      "[16:16] overcome this is instead of just giving\n",
      "[16:18] it to one LLM, you can give it to 10\n",
      "[16:20] LLMs. the same task of score this lead\n",
      "[16:23] and then you can aggregate the responses\n",
      "[16:27] of all 10 LLMs to give your final\n",
      "[16:30] result. The key upside here is that you\n",
      "[16:32] can improve the performance of your\n",
      "[16:35] agentic system while here you can\n",
      "[16:37] improve the speed of your agentic\n",
      "[16:40] system. Another common design is this\n",
      "[16:43] orchestrator workers paradigm where\n",
      "[16:46] instead of just having a rigid workflow\n",
      "[16:48] like we saw in the previous slide where\n",
      "[16:50] basically everything was predefined and\n",
      "[16:52] we have some agency within the LLM\n",
      "[16:55] modules because they have the freedom to\n",
      "[16:57] perform that specific task in whatever\n",
      "[16:59] way they like. This may not work for\n",
      "[17:02] tasks which are a bit less predictable\n",
      "[17:04] where you don't have a clear workflow\n",
      "[17:06] for all possible requests. In this\n",
      "[17:09] orchestrator workers design, you first\n",
      "[17:11] have the request go to a planning LLM.\n",
      "[17:14] This could be a really smart LLM that\n",
      "[17:16] can reason and think through problems.\n",
      "[17:18] And basically its only job is to come up\n",
      "[17:21] with the workflow to come up with the\n",
      "[17:24] plan of solving this problem or\n",
      "[17:26] performing this task. And then based on\n",
      "[17:28] that plan, the tasks are delegated to\n",
      "[17:32] separate systems. So to multiple LLMs or\n",
      "[17:35] maybe to some predefined code components\n",
      "[17:38] that tend to come up a lot for your\n",
      "[17:40] specific use case. So this is kind of\n",
      "[17:43] analogous to how you might have a single\n",
      "[17:45] LLM think step by step through a problem\n",
      "[17:49] and then solve the problem, but instead\n",
      "[17:51] of everything happening within one LLM,\n",
      "[17:53] you can spread this task among multiple\n",
      "[17:55] LLMs to potentially get better\n",
      "[17:57] performance. Finally, we have the\n",
      "[17:59] evaluator optimizer paradigm. The way\n",
      "[18:02] this works is that you have an LLM that\n",
      "[18:04] generates some response or some output\n",
      "[18:06] and then you have another system review\n",
      "[18:09] that response and give feedback. And\n",
      "[18:11] basically this loop can continue until\n",
      "[18:13] some stopping criteria are satisfied and\n",
      "[18:16] then it goes to the next step of the\n",
      "[18:19] workflow. This evaluator optimizer\n",
      "[18:21] pattern is fundamentally different than\n",
      "[18:24] all the other patterns that we're seeing\n",
      "[18:26] here. And the difference is that\n",
      "[18:28] everything else has a clear beginning\n",
      "[18:30] and end. It starts at point A and it\n",
      "[18:32] ends at point B and you know even if\n",
      "[18:34] you're splitting it up to operate in\n",
      "[18:36] parallel there's a clear end point.\n",
      "[18:39] However, because the evaluator optimizer\n",
      "[18:42] is this loop this could you know in\n",
      "[18:44] principle just go on indefinitely. So\n",
      "[18:47] inherently this system is capable of\n",
      "[18:50] doing tasks with more autonomy with more\n",
      "[18:52] agency than the other ones because we\n",
      "[18:55] haven't created these constraints or\n",
      "[18:59] rails for the system to operate on. It\n",
      "[19:02] basically figures out what it wants to\n",
      "[19:04] do by itself. This brings us to the\n",
      "[19:06] level three example which I call an LLM\n",
      "[19:09] in a loop. The idea here is giving an\n",
      "[19:12] LLM real world feedback on responses\n",
      "[19:15] until they are satisfactory. So to give\n",
      "[19:18] a concrete example, let's say I wanted\n",
      "[19:20] to have an LLM system, an AENTIC system\n",
      "[19:23] write LinkedIn posts for me. So I might\n",
      "[19:25] have an LLM writer. It's going to write\n",
      "[19:27] a LinkedIn post for me. And then what\n",
      "[19:29] I'm going to do is pass this post\n",
      "[19:30] through various sets of evaluations, you\n",
      "[19:33] know. So, the first thing I'm going to\n",
      "[19:34] do is see if that post is similar to my\n",
      "[19:37] existing posts because I want it to\n",
      "[19:40] sound like me and I want it to be\n",
      "[19:42] relevant to the other things that I've\n",
      "[19:43] talked about. Another evaluation I might\n",
      "[19:45] do is pass it through a AI post\n",
      "[19:48] detection system. That's because I don't\n",
      "[19:50] want the post to sound like an AI wrote\n",
      "[19:52] it. I want it to sound humanlike. And\n",
      "[19:54] then finally, I can evaluate if the\n",
      "[19:56] LinkedIn post is actually similar to the\n",
      "[19:59] initial idea that I fed into this LLM\n",
      "[20:03] system. And so then each of these\n",
      "[20:05] evaluations, we'll have like a pass\n",
      "[20:07] fail. And then we can have some logic\n",
      "[20:09] where if any of these evaluations\n",
      "[20:11] failed, we'll gather all the feedback\n",
      "[20:13] and we'll pass it back to the LLM\n",
      "[20:15] writer. We might say, okay, the post was\n",
      "[20:17] relevant to the idea, but it sounded AI\n",
      "[20:20] generated. you know, like 80% was AI\n",
      "[20:23] generated and it didn't really match the\n",
      "[20:25] voice of my existing posts. So, we kind\n",
      "[20:28] of gather all this feedback, give back\n",
      "[20:30] to the LLM, and then the LLM can go,\n",
      "[20:31] \"Oh, okay. So, I got this feedback.\n",
      "[20:33] Here's the post I just wrote. Let me\n",
      "[20:35] update it and write another one.\" And\n",
      "[20:37] then that'll spit out another LinkedIn\n",
      "[20:39] post. It'll go through the same\n",
      "[20:40] evaluations. And then again, if any of\n",
      "[20:42] them fail, we'll gather the feedback,\n",
      "[20:44] give it to the LLM, and continue in this\n",
      "[20:46] loop like this. And so the value of\n",
      "[20:47] these systems is that they can perform\n",
      "[20:49] very open-ended tasks. Let's say if I\n",
      "[20:52] wanted an LLM to write a LinkedIn post\n",
      "[20:53] for me, I might have to optimize the\n",
      "[20:56] prompt for a very specific idea or very\n",
      "[20:59] specific set of ideas for it to perform\n",
      "[21:01] well. While it might work well on those\n",
      "[21:03] examples, it's going to be fundamentally\n",
      "[21:06] limited because, you know, maybe it has\n",
      "[21:07] to follow a certain template or maybe it\n",
      "[21:09] needs like a reference document to write\n",
      "[21:12] the post from and all these other\n",
      "[21:14] constraints. But doing it in this way\n",
      "[21:16] where we're just giving the model\n",
      "[21:18] feedback, evaluating it based on these\n",
      "[21:21] outcomes that I care about, it can\n",
      "[21:23] potentially figure out the best post to\n",
      "[21:26] write through this closed loop. And then\n",
      "[21:28] if all the tests pass, we can return the\n",
      "[21:31] post. And so there's actually another\n",
      "[21:32] version of this. As I'm showing the\n",
      "[21:34] example here, we are just using the LLM\n",
      "[21:37] out of the box. And then we're just\n",
      "[21:38] getting very clever on how we define\n",
      "[21:40] these evaluations to design this system.\n",
      "[21:43] However, we could also do something\n",
      "[21:45] similar where we actually update the\n",
      "[21:48] internal parameters of the LLM. In other\n",
      "[21:50] words, we fine-tune the LLM to be\n",
      "[21:54] optimized in solving this task. So, the\n",
      "[21:57] model can get better and better the more\n",
      "[22:00] posts that it writes. This is the idea\n",
      "[22:02] of using end-to-end reinforcement\n",
      "[22:04] learning on realworld feedback in\n",
      "[22:07] optimizing an LLM system. The only\n",
      "[22:10] difference here is that we make the\n",
      "[22:12] parameters trainable and then we use an\n",
      "[22:14] algorithm like group relative policy\n",
      "[22:16] optimization to update the internal\n",
      "[22:18] parameters. And if you don't know what\n",
      "[22:20] I'm talking about with reinforcement\n",
      "[22:21] learning or group relative policy\n",
      "[22:23] optimization, I talked about that in a\n",
      "[22:25] previous video on reasoning models like\n",
      "[22:28] 01 and R1. While this might sound like\n",
      "[22:30] some kind of research project or\n",
      "[22:32] something like that, this is actually\n",
      "[22:33] the approach behind models like deep\n",
      "[22:36] research which is currently available in\n",
      "[22:38] chatbt and also openai's operator model\n",
      "[22:42] which is only available to prousers but\n",
      "[22:44] basically that model is capable of\n",
      "[22:47] surfing the web interacting with guies\n",
      "[22:49] and doing things on your behalf. Okay,\n",
      "[22:52] so this was actually the first video in\n",
      "[22:54] a larger series on AI agents. So here\n",
      "[22:57] the discussion was relatively high\n",
      "[22:59] level. We didn't go into the technical\n",
      "[23:01] details or write any code. That's\n",
      "[23:03] because I just wanted to lay the\n",
      "[23:04] foundation for future videos where I'm\n",
      "[23:07] going to talk through each of these one\n",
      "[23:09] by one and build concrete examples of\n",
      "[23:12] each of them. And so in the next video\n",
      "[23:14] I'll be building a level one agentic\n",
      "[23:17] system. I haven't decided what I'm going\n",
      "[23:18] to build yet. So if you have any\n",
      "[23:19] suggestions let me know in the comment\n",
      "[23:21] section below. Also if there's anything\n",
      "[23:22] else you want me to cover in this series\n",
      "[23:24] let me know in the comments. The AI\n",
      "[23:26] agent space is massive and it's only\n",
      "[23:28] growing and I'm sure there's so many\n",
      "[23:30] tools and ideas out there that I'm\n",
      "[23:32] completely ignorant of. And so if\n",
      "[23:34] there's anything you keep seeing over\n",
      "[23:35] and over again and you think I should\n",
      "[23:36] cover, let me know. And with that, as\n",
      "[23:38] always, thank you so much for your time\n",
      "[23:39] and thanks for watching.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The transcript for the video from 00:00 to 00:15 is:\n",
      "\n",
      "[00:00] Hey everyone, I'm Shaw. This is the\n",
      "[00:02] first video in a larger series on AI\n",
      "[00:05] agents. Here I'll start by discussing\n",
      "[00:07] what an AI agent actually is and why we\n",
      "[00:10] should care about them. Then I'll review\n",
      "[00:12] three common types of agents at various\n",
      "[00:15] levels of agency.\n",
      "\n",
      "The transcript is already in English.\n"
     ]
    }
   ],
   "source": [
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
