{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search & RAG with LlamaIndex\n",
    "## ABB #5 - Session 3\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from typing import Optional, Dict, Any\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596f6cd-47b2-41d9-905e-a5b8862cf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "#my_sk = os.getenv(\"OPENAI_API_KEY_PERSONAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_text(url: str) -> str:\n",
    "    \"\"\"Fetch and extract visible text from a web page.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Remove script and style elements\n",
    "        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            script_or_style.decompose()\n",
    "        # Get text and clean up whitespace\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        # Remove excessive whitespace\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching {url}: {e}\"\n",
    "\n",
    "def serpapi_search_and_scrape(query: str, num_results: int = 2) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Perform a Google search using SerpAPI, get top result links,\n",
    "    fetch and return their page text.\n",
    "    Returns a list of dicts: [{\"url\": ..., \"text\": ...}, ...]\n",
    "    \"\"\"\n",
    "    api_key = os.environ.get(\"SERP_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"SERP_API_KEY environment variable not set.\")\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"num\": num_results,\n",
    "    }\n",
    "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    results = []\n",
    "    organic_results = data.get(\"organic_results\", [])[:num_results]\n",
    "    for result in organic_results:\n",
    "        link = result.get(\"link\")\n",
    "        if link:\n",
    "            page_text = fetch_page_text(link)\n",
    "            results.append({\"url\": link, \"text\": page_text})\n",
    "    return results\n",
    "\n",
    "def test_serpapi_search_and_scrape():\n",
    "    results = serpapi_search_and_scrape(\"What is fine-tuning?\")\n",
    "    for idx, res in enumerate(results):\n",
    "        print(f\"Result {idx+1}: {res['url']}\\nText (first 500 chars):\\n{res['text'][:500]}\\n{'-'*60}\")\n",
    "\n",
    "test_serpapi_search_and_scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d779f30-2fb3-4292-a627-fdec718d6767",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [\"articles/\"+f for f in os.listdir('articles')]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "    # only process .html files\n",
    "    if filename.lower().endswith(('.html')):\n",
    "        # read html file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Get article title\n",
    "        article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        article_content = []\n",
    "        current_section = \"Main\"  # Default section if no headers found\n",
    "        \n",
    "        # Find all headers and text content\n",
    "        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "    \n",
    "        # iterate through elements and extract text with metadata\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                current_section = element.get_text().strip()\n",
    "            elif element.name in ['p', 'ul', 'ol']:\n",
    "                text = element.get_text().strip()\n",
    "                # Only add non-empty content that's at least 30 characters long\n",
    "                if text and len(text) >= 30:\n",
    "                    article_content.append({\n",
    "                        'article_title': article_title,\n",
    "                        'section': current_section,\n",
    "                        'text': text\n",
    "                    })\n",
    "    \n",
    "        # add article content to list\n",
    "        chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f75f3-7f71-4c5f-afb5-c9f077cfe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes with Llama Index (i.e. nodes)\n",
    "node_list = []\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    node_list.append(\n",
    "        TextNode(\n",
    "            id_=str(i), \n",
    "            text=chunk[\"text\"], \n",
    "            metadata = {\n",
    "                \"article\":chunk[\"article_title\"],\n",
    "                \"section\":chunk[\"section\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(len(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08452dc6-24ee-4cd7-a0ce-fbb7998761fe",
   "metadata": {},
   "source": [
    "### 2) create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239d39b-3953-46d3-b2f1-840fc6df076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses OpenAI API key which is rate limited whn I run it, so commented out to use HuggingFace embedding which is free (see below)\n",
    "#index = VectorStoreIndex(node_list)\n",
    "#print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "#print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "#print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2e5f9-ce1b-4b50-931c-367c2f961948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# changing embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fad49-b003-4a96-9e3e-e89e33854a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(node_list)\n",
    "\n",
    "print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc70eb0-019e-424c-9d84-7522b83647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.retrieve(\"What is the main topic of the article?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd93b0f-a618-43cb-a43a-2841697724a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.retrieve(\"When do I perform fine-tuning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1936aff-9604-4bd1-a6e2-a4d42bc84d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d5934-c0d8-4995-bdf2-4741c68443a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format results in markdown\n",
    "results_markdown = \"\"\n",
    "for i, result in enumerate(results, start=1):\n",
    "    results_markdown += f\"{i}. **Article title:** {result.metadata[\"article\"]}  \\n\"\n",
    "    results_markdown += f\"   **Section:** {result.metadata[\"section\"]}  \\n\"\n",
    "    results_markdown += f\"   **Snippet:** {result.text} \\n\\n\"\n",
    "    results_markdown += f\"   **Score:** {result.score} \\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9281d63-1cd9-4f20-b18c-c6d556eba289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce449aa0",
   "metadata": {},
   "source": [
    "#### Change the default model to use ollama\n",
    "* Ensure ``ollama serve`` works before using the local open source ollama model\n",
    "\n",
    "* ``ollama list`` to see which local models are available. Thenpick one from the list as shown in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Switch to a local LLM (Ollama, e.g., llama3) to avoid OpenAI rate limits\n",
    "Settings.llm = Ollama(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62772048-c152-4e5b-9afd-6e7fa4bc9e73",
   "metadata": {},
   "source": [
    "### 4) RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5730c-028e-424a-a0e4-b7ee5f445061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code demonstrates how to perform a Retrieval-Augmented Generation (RAG) query using only the data that has been previously indexed (i.e., the Nodes from your document collection).\n",
    "# \n",
    "# 1. The `get_response_synthesizer()` function configures how the language model will synthesize answers, but it does not provide any new data to the model.\n",
    "# 2. `index.as_query_engine(response_synthesizer=response_synthesizer)` creates a query engine that is strictly limited to the indexed Nodesâ€”these are the chunks of your ingested articles.\n",
    "# 3. When you call `query_engine.query(\"What is the main topic of the article?\")`, the engine retrieves relevant Nodes from the index and passes their content to the LLM for answer synthesis.\n",
    "# 4. Importantly, the LLM does not have access to any external data, web search, or its own training corpus for factual retrieval. It can only generate answers based on the text of the indexed Nodes.\n",
    "# 5. The result printed is therefore grounded solely in the content of your indexed articles, ensuring that the response is limited to your data and not influenced by information outside your corpus.\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# create a query engine that only uses the indexed Nodes (no external or model-intrinsic data)\n",
    "query_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\n",
    "\n",
    "# query the index: the response is generated strictly from the indexed Nodes\n",
    "test_response = query_engine.query('When do I perform fine-tuning?') #(\"What is the main topic of the article?\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15191300",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630ecca-ebe8-4363-b0a7-511f7c0e1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc1b5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during augmented RAG query: timed out\n",
      "Test failed: timed out\n"
     ]
    }
   ],
   "source": [
    "def synthesize_final_answer(rag_answer: str, serp_snippets: List[str], query: str, llm) -> str:\n",
    "    \"\"\"Combine RAG and Google search results, then synthesize a final answer using the LLM.\"\"\"\n",
    "    context = (\n",
    "        f\"RAG answer (from indexed documents):\\n{rag_answer}\\n\\n\"\n",
    "        f\"Google Search Results:\\n\"\n",
    "        + \"\\n\".join(f\"- {s}\" for s in serp_snippets)\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"Given the following question:\\n{query}\\n\\n\"\n",
    "        f\"Here is an answer based on a private document collection, and some recent Google search results.\\n\"\n",
    "        f\"Document-based answer:\\n{rag_answer}\\n\\n\"\n",
    "        f\"Google search snippets:\\n\"\n",
    "        + \"\\n\".join(f\"- {s}\" for s in serp_snippets)\n",
    "        + \"\\n\\n\"\n",
    "        \"Please synthesize a final, concise, and accurate answer to the question, using both sources. \"\n",
    "        \"If the sources disagree, explain the difference.\"\n",
    "    )\n",
    "    # Use the LLM to synthesize the final answer\n",
    "    return llm.complete(prompt).text\n",
    "\n",
    "def augmented_rag_query(\n",
    "    query: str,\n",
    "    rag_query_engine,\n",
    "    llm,\n",
    "    serpapi_num_results: int = 3\n",
    ") -> str:\n",
    "    \"\"\"Perform RAG, augment with Google search, and synthesize a final answer.\"\"\"\n",
    "    # Step 1: RAG answer\n",
    "    rag_response = rag_query_engine.query(query)\n",
    "    rag_answer = str(rag_response)\n",
    "    # Step 2: Google search\n",
    "    serp_snippets = serpapi_search_and_scrape(query, num_results=serpapi_num_results)\n",
    "    serp_snippets_text_only = [snippet[\"text\"] for snippet in serp_snippets]\n",
    "    # Step 3: Synthesize final answer\n",
    "    return synthesize_final_answer(rag_answer, serp_snippets_text_only, query, llm)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    test_query = \"When do I perform fine-tuning?\"\n",
    "    try:\n",
    "        final_answer = augmented_rag_query(\n",
    "            test_query,\n",
    "            query_engine,\n",
    "            Settings.llm,\n",
    "            serpapi_num_results=3\n",
    "        )\n",
    "        print(\"Augmented RAG + Google Search Answer:\\n\", final_answer)\n",
    "    except Exception as e:\n",
    "        print(\"Error during augmented RAG query:\", e)\n",
    "\n",
    "# Test: Check that the function returns a string and includes both RAG and Google content\n",
    "def test_augmented_rag_query_integration():\n",
    "    test_query = \"What is Retrieval-Augmented Generation?\"\n",
    "    try:\n",
    "        answer = augmented_rag_query(test_query, query_engine, Settings.llm, serpapi_num_results=2)\n",
    "        assert isinstance(answer, str)\n",
    "        assert len(answer) > 0\n",
    "        print(\"Test passed: augmented_rag_query returns a non-empty string.\")\n",
    "    except Exception as e:\n",
    "        print(\"Test failed:\", e)\n",
    "\n",
    "test_augmented_rag_query_integration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bec48c-41c6-4539-8cae-572c83827362",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ffd35-8859-4215-815b-23f4cf6d5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d6739-d6e3-4480-a09a-91117c3cf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# changing the global LLM\n",
    "Settings.llm = OpenAI(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49a2e8-67e0-4adf-8cf8-cda579333312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler way to make query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9485a01-a32a-47fa-8e12-e4be9fef595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
