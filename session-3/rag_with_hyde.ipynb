{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search & RAG with LlamaIndex\n",
    "## ABB #5 - Session 3\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596f6cd-47b2-41d9-905e-a5b8862cf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "my_sk = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d779f30-2fb3-4292-a627-fdec718d6767",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [\"articles/\"+f for f in os.listdir('articles')]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "    # only process .html files\n",
    "    if filename.lower().endswith(('.html')):\n",
    "        # read html file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Get article title\n",
    "        article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        article_content = []\n",
    "        current_section = \"Main\"  # Default section if no headers found\n",
    "        \n",
    "        # Find all headers and text content\n",
    "        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "    \n",
    "        # iterate through elements and extract text with metadata\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                current_section = element.get_text().strip()\n",
    "            elif element.name in ['p', 'ul', 'ol']:\n",
    "                text = element.get_text().strip()\n",
    "                # Only add non-empty content that's at least 30 characters long\n",
    "                if text and len(text) >= 30:\n",
    "                    article_content.append({\n",
    "                        'article_title': article_title,\n",
    "                        'section': current_section,\n",
    "                        'text': text\n",
    "                    })\n",
    "    \n",
    "        # add article content to list\n",
    "        chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f75f3-7f71-4c5f-afb5-c9f077cfe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes with Llama Index (i.e. nodes)\n",
    "node_list = []\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    node_list.append(\n",
    "        TextNode(\n",
    "            id_=str(i), \n",
    "            text=chunk[\"text\"], \n",
    "            metadata = {\n",
    "                \"article\":chunk[\"article_title\"],\n",
    "                \"section\":chunk[\"section\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(len(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08452dc6-24ee-4cd7-a0ce-fbb7998761fe",
   "metadata": {},
   "source": [
    "### 2) create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239d39b-3953-46d3-b2f1-840fc6df076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(node_list)\n",
    "\n",
    "print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2e5f9-ce1b-4b50-931c-367c2f961948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# changing embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from typing import Callable\n",
    "\n",
    "def create_hyde_prompt(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a hyDE-style prompt for a given question using LangChain's PromptTemplate.\n",
    "    Pure function: no side effects.\n",
    "    \"\"\"\n",
    "    template = (\n",
    "        \"You are given a user question. Carefully analyze its intent and semantic meaning. \"\n",
    "        \"Generate a detailed, plausible answer that directly addresses the question, \"\n",
    "        \"using relevant terminology and context. This hypothetical answer should be as informative and specific as possible, \"\n",
    "        \"to maximize the chance of retrieving documents that truly match the user's information need.\\n\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Hypothetical Answer:\"\n",
    "    )\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=template\n",
    "    )\n",
    "    return prompt.format(question=question)\n",
    "\n",
    "def get_ollama_llm(model: str = \"llama3.2:latest\") -> Callable[[str], str]:\n",
    "    \"\"\"\n",
    "    Factory function to create an OllamaLLM instance with the given model.\n",
    "    Returns a function that takes a prompt and returns the LLM's response.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=model)\n",
    "    def invoke(prompt: str) -> str:\n",
    "        # Use the new .invoke method as per deprecation warning\n",
    "        return llm.invoke(prompt)\n",
    "    return invoke\n",
    "\n",
    "def generate_hypothetical_document(question: str, model: str = \"llama3.2:latest\") -> str:\n",
    "    \"\"\"\n",
    "    Uses OllamaLLM to generate a hypothetical document for the given question.\n",
    "    Pure function: no side effects except for printing.\n",
    "    \"\"\"\n",
    "    prompt = create_hyde_prompt(question)\n",
    "    #print(prompt)\n",
    "    ollama_invoke = get_ollama_llm(model)\n",
    "    return ollama_invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fad49-b003-4a96-9e3e-e89e33854a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(node_list)\n",
    "\n",
    "print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc70eb0-019e-424c-9d84-7522b83647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8952497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_hyde(\n",
    "    query: str,\n",
    "    retriever,\n",
    "    hyde_llm: str = \"llama3.2:latest\"\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Given a query, generate a hypothetical document using HyDE and retrieve relevant documents using the retriever.\n",
    "    Returns the retrieval results.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate hypothetical document\n",
    "    hypothetical_doc = generate_hypothetical_document(query, model=hyde_llm)\n",
    "    #print(\"Hypothetical Document Generated:\\n\", hypothetical_doc)\n",
    "\n",
    "    # Step 2: Retrieve relevant documents using the hypothetical document as the query\n",
    "    results = retriever.retrieve(hypothetical_doc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd93b0f-a618-43cb-a43a-2841697724a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.retrieve(\"When do I perform fine-tuning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_retrieved_results(results) -> None:\n",
    "    print(results[0])\n",
    "    # format results in markdown\n",
    "    results_markdown = \"\"\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        results_markdown += f\"{i}. **Article title:** {result.metadata[\"article\"]}  \\n\"\n",
    "        results_markdown += f\"   **Section:** {result.metadata[\"section\"]}  \\n\"\n",
    "        results_markdown += f\"   **Snippet:** {result.text} \\n\\n\"\n",
    "        results_markdown += f\"   **Score:** {result.score} \\n\\n\"\n",
    "    display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf78931",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_retrieved_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aaedb6",
   "metadata": {},
   "source": [
    "#### With hyde dummy document augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af58216",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieve_with_hyde(\"When do I perform fine-tuning?\", retriever)\n",
    "display_retrieved_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62772048-c152-4e5b-9afd-6e7fa4bc9e73",
   "metadata": {},
   "source": [
    "### 4) RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5730c-028e-424a-a0e4-b7ee5f445061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630ecca-ebe8-4363-b0a7-511f7c0e1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine with hyde\n",
    "from typing import Any, List\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "\n",
    "class HydeRetriever(BaseRetriever):\n",
    "    def __init__(self, retriever_func, base_retriever):\n",
    "        self._retriever_func = retriever_func\n",
    "        self._base_retriever = base_retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle, **kwargs: Any) -> List[NodeWithScore]:\n",
    "        # delegate to the provided function, which should return List[NodeWithScore]\n",
    "        return self._retriever_func(query_bundle.query_str, self._base_retriever)\n",
    "\n",
    "# Wrap retrieve_with_hyde in a class that implements .retrieve()\n",
    "hyde_retriever = HydeRetriever(retrieve_with_hyde, retriever)\n",
    "\n",
    "query_engine_hyde = RetrieverQueryEngine(\n",
    "    retriever=hyde_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "response = query_engine_hyde.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bec48c-41c6-4539-8cae-572c83827362",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ffd35-8859-4215-815b-23f4cf6d5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d6739-d6e3-4480-a09a-91117c3cf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# changing the global LLM\n",
    "Settings.llm = OpenAI(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49a2e8-67e0-4adf-8cf8-cda579333312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler way to make query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9485a01-a32a-47fa-8e12-e4be9fef595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
