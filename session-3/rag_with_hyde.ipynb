{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search & RAG with LlamaIndex & HyDE prompt\n",
    "## ABB #5 - Session 3\n",
    "\n",
    "Code authored by: Shaw Talebi\n",
    "\n",
    "Modified by: Barry Mukund"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b486f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for ImportError: cannot import name 'Mapping' from 'collections'\n",
    "# This error is often caused by dependencies that expect 'Mapping' in 'collections' (pre-3.10),\n",
    "# but in Python 3.10+ it is in 'collections.abc'. If you encounter this error, \n",
    "# ensure all dependencies are up to date. If not possible, patch before import.\n",
    "\n",
    "import collections\n",
    "import collections.abc\n",
    "import sys\n",
    "\n",
    "# Patch 'collections' to have 'Mapping' if missing (for legacy dependencies)\n",
    "if not hasattr(collections, 'Mapping'):\n",
    "    collections.Mapping = collections.abc.Mapping\n",
    "\n",
    "from llama_index.core.indices.vector_store import VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1596f6cd-47b2-41d9-905e-a5b8862cf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "my_sk = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0ea4b",
   "metadata": {},
   "source": [
    "#### Setup embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ef8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# changing embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    show_progress_bar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b2a30",
   "metadata": {},
   "source": [
    "#### Use ollama as openai runs into ratelimit due to IP being blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5180be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: llama3.2:latest\n"
     ]
    }
   ],
   "source": [
    "# changing the global LLM\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "Settings.llm = Ollama(model=\"llama3.2:latest\")\n",
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d779f30-2fb3-4292-a627-fdec718d6767",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947c1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_file_path='/Users/barry/ai_tut/cohort/AI-Builders-Bootcamp-5/session-3/articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [f\"{articles_file_path}/{f}\" for f in os.listdir(articles_file_path)]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "    # only process .html files\n",
    "    if filename.lower().endswith(('.html')):\n",
    "        # read html file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Get article title\n",
    "        article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        article_content = []\n",
    "        current_section = \"Main\"  # Default section if no headers found\n",
    "        \n",
    "        # Find all headers and text content\n",
    "        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "    \n",
    "        # iterate through elements and extract text with metadata\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                current_section = element.get_text().strip()\n",
    "            elif element.name in ['p', 'ul', 'ol']:\n",
    "                text = element.get_text().strip()\n",
    "                # Only add non-empty content that's at least 30 characters long\n",
    "                if text and len(text) >= 30:\n",
    "                    article_content.append({\n",
    "                        'article_title': article_title,\n",
    "                        'section': current_section,\n",
    "                        'text': text\n",
    "                    })\n",
    "    \n",
    "        # add article content to list\n",
    "        chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37be024",
   "metadata": {},
   "source": [
    "#### Create LLAMA index nodes based on the chunked text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3f75f3-7f71-4c5f-afb5-c9f077cfe523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "# create nodes with Llama Index (i.e. nodes)\n",
    "node_list = []\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    node_list.append(\n",
    "        TextNode(\n",
    "            id_=str(i), \n",
    "            text=chunk[\"text\"], \n",
    "            metadata = {\n",
    "                \"article\":chunk[\"article_title\"],\n",
    "                \"section\":chunk[\"section\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(len(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08452dc6-24ee-4cd7-a0ce-fbb7998761fe",
   "metadata": {},
   "source": [
    "### 2) create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0239d39b-3953-46d3-b2f1-840fc6df076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model: BAAI/bge-small-en-v1.5\n",
      "Index Size: 778\n",
      "Embedding Size: 384\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(node_list)\n",
    "\n",
    "print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0d657",
   "metadata": {},
   "source": [
    "#### Setup basic functions for hyDE prompting\n",
    "In this session, in order to do RAG retreival accurately, we take in the user prompt, then push it thru some LLM (ollama in this case), to generate a \n",
    "hypothetical document that contains more information based on the user query. This is then embedded and articles in VectorDb close to this hypothetical\n",
    "document is retrieved.\n",
    "\n",
    "I compared simple retrieval based solely on the prompt given and retrieval based on hypothetical document created and found that retrieval based on hypothetical document had better similarity scores (~0.8 for simple retrieval versus ~0.9 for hypothetical document based retrieval)\n",
    "\n",
    "This concept is discussed here: https://aclanthology.org/2023.acl-long.99/\n",
    "\n",
    "Python notebook: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3077651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from typing import Callable\n",
    "\n",
    "def create_hyde_prompt(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a hyDE-style prompt for a given question using LangChain's PromptTemplate.\n",
    "    Pure function: no side effects.\n",
    "    \"\"\"\n",
    "    template = (\n",
    "        \"You are given a user question. Carefully analyze its intent and semantic meaning. \"\n",
    "        \"Generate a detailed, plausible answer that directly addresses the question, \"\n",
    "        \"using relevant terminology and context. This hypothetical answer should be as informative and specific as possible, \"\n",
    "        \"to maximize the chance of retrieving documents that truly match the user's information need.\\n\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Hypothetical Answer:\"\n",
    "    )\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=template\n",
    "    )\n",
    "    return prompt.format(question=question)\n",
    "\n",
    "def get_ollama_llm(model: str = \"llama3.2:latest\") -> Callable[[str], str]:\n",
    "    \"\"\"\n",
    "    Factory function to create an OllamaLLM instance with the given model.\n",
    "    Returns a function that takes a prompt and returns the LLM's response.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=model)\n",
    "    def invoke(prompt: str) -> str:\n",
    "        # Use the new .invoke method as per deprecation warning\n",
    "        return llm.invoke(prompt)\n",
    "    return invoke\n",
    "\n",
    "def generate_hypothetical_document(question: str, model: str = \"llama3.2:latest\") -> str:\n",
    "    \"\"\"\n",
    "    Uses OllamaLLM to generate a hypothetical document for the given question.\n",
    "    Pure function: no side effects except for printing.\n",
    "    \"\"\"\n",
    "    prompt = create_hyde_prompt(question)\n",
    "    #print(prompt)\n",
    "    ollama_invoke = get_ollama_llm(model)\n",
    "    return ollama_invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) RAG retreival using semantic search (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc70eb0-019e-424c-9d84-7522b83647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfb4cc",
   "metadata": {},
   "source": [
    "#### RAG retrieval with HyDE based hypothetical document generation, then embedding it and looking to retrieve similar articles from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8952497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_hyde(\n",
    "    query: str,\n",
    "    retriever,\n",
    "    hyde_llm: str = \"llama3.2:latest\"\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Given a query, generate a hypothetical document using HyDE and retrieve relevant documents using the retriever.\n",
    "    Returns the retrieval results.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate hypothetical document\n",
    "    hypothetical_doc = generate_hypothetical_document(query, model=hyde_llm)\n",
    "    #print(\"Hypothetical Document Generated:\\n\", hypothetical_doc)\n",
    "\n",
    "    # Step 2: Retrieve relevant documents using the hypothetical document as the query\n",
    "    results = retriever.retrieve(hypothetical_doc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5310ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_retrieved_results(results) -> None:\n",
    "    \"\"\"\n",
    "    Display retrieved results in markdown format.\n",
    "    \"\"\"\n",
    "    print(results[0])\n",
    "    # format results in markdown\n",
    "    results_markdown = \"\"\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        results_markdown += f\"{i}. **Article title:** {result.metadata[\"article\"]}  \\n\"\n",
    "        results_markdown += f\"   **Section:** {result.metadata[\"section\"]}  \\n\"\n",
    "        results_markdown += f\"   **Snippet:** {result.text} \\n\\n\"\n",
    "        results_markdown += f\"   **Score:** {result.score} \\n\\n\"\n",
    "    display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c923e3",
   "metadata": {},
   "source": [
    "#### Non HyDE based RAG retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fd93b0f-a618-43cb-a43a-2841697724a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 155\n",
      "Text: This is not to say that fine-tuning is useless. A central\n",
      "benefit of fine-tuning an AI assistant is lowering inference costs\n",
      "[3].\n",
      "Score:  0.811\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When do I Fine-tune?  \n",
       "   **Snippet:** This is not to say that fine-tuning is useless. A central benefit of fine-tuning an AI assistant is lowering inference costs [3]. \n",
       "\n",
       "   **Score:** 0.8114657628676825 \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When NOT to Fine-tune  \n",
       "   **Snippet:** The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1]. \n",
       "\n",
       "   **Score:** 0.8002938091806874 \n",
       "\n",
       "3. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** How to Prepare Data for Fine-tuning?  \n",
       "   **Snippet:** For example, if I wanted to fine-tune an LLM to respond to viewer questions on YouTube, I would need to gather a set of comments with questions and my associated responses. For a concrete example of this, check out the code walk-through on YouTube. \n",
       "\n",
       "   **Score:** 0.7996616011957226 \n",
       "\n",
       "4. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When do I Fine-tune?  \n",
       "   **Snippet:** Fine-tuning, on the other hand, can compress prompt sizes by directly training the model on examples. Shorter prompts mean fewer tokens at inference, leading to lower compute costs and faster model responses [3]. For instance, after fine-tuning, the above prompt could be compressed to the following. \n",
       "\n",
       "   **Score:** 0.7995040812236383 \n",
       "\n",
       "5. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best. \n",
       "\n",
       "   **Score:** 0.7930143949465129 \n",
       "\n",
       "6. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** 3 Ways to Fine-tune  \n",
       "   **Snippet:** The next, and perhaps most popular, way to fine-tune a model is via supervised learning. This involves training a model on input-output pairs for a particular task. An example is instruction tuning, which aims to improve model performance in answering questions or responding to user prompts [1,3]. \n",
       "\n",
       "   **Score:** 0.7919754263499469 \n",
       "\n",
       "7. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1]. \n",
       "\n",
       "   **Score:** 0.7899394659656693 \n",
       "\n",
       "8. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** What is Fine-tuning?  \n",
       "   **Snippet:** Fine-tuning is taking a pre-trained model and training at least one internal model parameter (i.e. weights). In the context of LLMs, what this typically accomplishes is transforming a general-purpose base model (e.g. GPT-3) into a specialized model for a particular use case (e.g. ChatGPT) [1]. \n",
       "\n",
       "   **Score:** 0.7895567848964524 \n",
       "\n",
       "9. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** What’s Next?  \n",
       "   **Snippet:** Here, I summarized the most common fine-tuning questions I’ve received over the past 12 months. While fine-tuning is not a panacea for all LLM use cases, it has key benefits. \n",
       "\n",
       "   **Score:** 0.7862102117718668 \n",
       "\n",
       "10. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** What is Fine-tuning?  \n",
       "   **Snippet:** I like to define fine-tuning as taking an existing (pre-trained) model and training at least 1 model parameter to adapt it to a particular use case. \n",
       "\n",
       "   **Score:** 0.785435251961344 \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = retriever.retrieve(\"When do I perform fine-tuning?\")\n",
    "display_retrieved_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aaedb6",
   "metadata": {},
   "source": [
    "#### hyDE based document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7af58216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 1\n",
      "Text: Fine-tuning involves adapting a pre-trained model to a\n",
      "particular use case through additional training.\n",
      "Score:  0.880\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** Fine-Tuning BERT for Text Classification  \n",
       "   **Section:** Fine-tuning  \n",
       "   **Snippet:** Fine-tuning involves adapting a pre-trained model to a particular use case through additional training. \n",
       "\n",
       "   **Score:** 0.8800494856674508 \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** Advanced Fine-tuning  \n",
       "   **Snippet:** Another way we can fine-tune language models is for classification tasks, such as classifying support ticket tiers, detecting spam emails, or determining the sentiment of a customer review. A classic fine-tuning approach for this is called transfer learning, where we replace the head of a language model to perform a new classification task. \n",
       "\n",
       "   **Score:** 0.8694429632303379 \n",
       "\n",
       "3. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** What is Fine-tuning?  \n",
       "   **Snippet:** Fine-tuning is taking a pre-trained model and training at least one internal model parameter (i.e. weights). In the context of LLMs, what this typically accomplishes is transforming a general-purpose base model (e.g. GPT-3) into a specialized model for a particular use case (e.g. ChatGPT) [1]. \n",
       "\n",
       "   **Score:** 0.8673302403269113 \n",
       "\n",
       "4. **Article title:** Fine-Tuning BERT for Text Classification  \n",
       "   **Section:** Conclusion  \n",
       "   **Snippet:** Fine-tuning pre-trained models is a powerful paradigm for developing better models at a lower cost than training them from scratch. Here, we saw how to do this with BERT using the Hugging Face Transformers library. \n",
       "\n",
       "   **Score:** 0.8644245090324031 \n",
       "\n",
       "5. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** 3 Ways to Fine-tune  \n",
       "   **Snippet:** The next, and perhaps most popular, way to fine-tune a model is via supervised learning. This involves training a model on input-output pairs for a particular task. An example is instruction tuning, which aims to improve model performance in answering questions or responding to user prompts [1,3]. \n",
       "\n",
       "   **Score:** 0.8635334401448006 \n",
       "\n",
       "6. **Article title:** Fine-Tuning BERT for Text Classification  \n",
       "   **Section:** Fine-tuning  \n",
       "   **Snippet:** Pre-trained models are developed via unsupervised learning, which precludes the need for large-scale labeled datasets. Fine-tuned models can then exploit pre-trained model representations to significantly reduce training costs and improve model performance compared to training from scratch [1]. \n",
       "\n",
       "   **Score:** 0.8591626484758503 \n",
       "\n",
       "7. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** What is Fine-tuning?  \n",
       "   **Snippet:** The key upside of this approach is that models can achieve better performance while requiring (far) fewer manually labeled examples compared to models that solely rely on supervised training. \n",
       "\n",
       "   **Score:** 0.8545674847194933 \n",
       "\n",
       "8. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** Conclusions  \n",
       "   **Snippet:** While fine-tuning an existing model requires more computational resources and technical expertise than using one out-of-the-box, (smaller) fine-tuned models can outperform (larger) pre-trained base models for a particular use case, even when employing clever prompt engineering strategies. Furthermore, with all the open-source LLM resources available, it’s never been easier to fine-tune a model for a custom application. \n",
       "\n",
       "   **Score:** 0.8485122703198279 \n",
       "\n",
       "9. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** Supervised Fine-tuning Steps (High-level)  \n",
       "   **Snippet:** Choose fine-tuning task (e.g. summarization, question answering, text classification)Prepare training dataset i.e. create (100–10k) input-output pairs and preprocess data (i.e. tokenize, truncate, and pad text).Choose a base model (experiment with different models and choose one that performs best on the desired task).Fine-tune model via supervised learningEvaluate model performance \n",
       "\n",
       "   **Score:** 0.845559047908106 \n",
       "\n",
       "10. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** 3 Ways to Fine-tune  \n",
       "   **Snippet:** Generate high-quality prompt-response pairs and fine-tune a pre-trained model using supervised learning. (~13k training prompts) Note: One can (alternatively) skip to step 2 with the pre-trained model [3].Use the fine-tuned model to generate completions and have human-labelers rank responses based on their preferences. Use these preferences to train the reward model. (~33k training prompts)Use the reward model and an RL algorithm (e.g. PPO) to fine-tune the model further. (~31k training prompts) \n",
       "\n",
       "   **Score:** 0.8441634112610668 \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = retrieve_with_hyde(\"When do I perform fine-tuning?\", retriever)\n",
    "display_retrieved_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62772048-c152-4e5b-9afd-6e7fa4bc9e73",
   "metadata": {},
   "source": [
    "### 4) RAG pipeline putting the flow together\n",
    "1. Setup how to format output using response_synthesizer\n",
    "2. Provide a pipeline with retriever, response format and similarity cut-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b5730c-028e-424a-a0e4-b7ee5f445061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb974137",
   "metadata": {},
   "source": [
    "#### RAG retriever flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5630ecca-ebe8-4363-b0a7-511f7c0e1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081da1ec",
   "metadata": {},
   "source": [
    "#### Now query using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24faed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning typically occurs when you want to take an existing (pre-trained) model and train at least one internal model parameter to adapt it to a particular use case. This process transforms a general-purpose base model into a specialized model for that specific purpose, resulting in compressed prompt sizes and lower inference costs.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87389d7a",
   "metadata": {},
   "source": [
    "#### HyDE based retriever pipeline\n",
    "1. First in order to standardize the pipeline, we need to subclass BaseRetriever and override _retrieve() that is called by the pipeline to get the RAG articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd3b05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine with hyde\n",
    "from typing import Any, List\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "\n",
    "class HydeRetriever(BaseRetriever):\n",
    "    def __init__(self, retriever_func, base_retriever):\n",
    "        self._retriever_func = retriever_func\n",
    "        self._base_retriever = base_retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle, **kwargs: Any) -> List[NodeWithScore]:\n",
    "        # delegate to the provided function, which should return List[NodeWithScore]\n",
    "        return self._retriever_func(query_bundle.query_str, self._base_retriever)\n",
    "\n",
    "# Wrap retrieve_with_hyde in a class that implements .retrieve()\n",
    "hyde_retriever = HydeRetriever(retrieve_with_hyde, retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2874920",
   "metadata": {},
   "source": [
    "#### Now Query using HyDE based retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a154c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning typically involves adapting a pre-trained model to a particular use case through additional training. This is often done when you want to transform a general-purpose base model into a specialized model for a specific task or application. You may need to perform fine-tuning in situations where you require better performance on a particular task while requiring fewer manually labeled examples than traditional supervised training methods.\n"
     ]
    }
   ],
   "source": [
    "query_engine_hyde = RetrieverQueryEngine(\n",
    "    retriever=hyde_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "response = query_engine_hyde.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813496c",
   "metadata": {},
   "source": [
    "#### Non-pipeline based simple RAG query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44684afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typically perform fine-tuning when you need to adapt a pre-trained AI assistant to a specific task or domain where its existing capabilities are not sufficient. This can help improve the model's performance and efficiency. Fine-tuning is often used in situations where the goal is to optimize the model for a particular use case, such as lowering inference costs.\n"
     ]
    }
   ],
   "source": [
    "# simpler way to make query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
